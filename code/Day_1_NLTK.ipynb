{"cells":[{"cell_type":"markdown","id":"experimental-loading","metadata":{"id":"experimental-loading"},"source":["Natural Language ToolKit (NLTK) \n","La libreria maggiormente utilizzata, specialmente in passato nell'ambito accademico. \n","Sviluppata dall'università della Pennsylvania, da Stefan Bird e Edward Loper. \n","\n","NLTK prende in input stringhe come oggetti e non é implementato per transformare parole/frasi in vettori. "]},{"cell_type":"code","execution_count":1,"id":"configured-thumb","metadata":{"id":"configured-thumb","executionInfo":{"status":"ok","timestamp":1642519623182,"user_tz":-60,"elapsed":2737,"user":{"displayName":"KNOTS","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEorsxhplCeVDMb5ALqZ2xJWG-NY8iKO4ZWlgF=s64","userId":"07911031499750462727"}}},"outputs":[],"source":["import nltk\n","import csv"]},{"cell_type":"code","execution_count":2,"id":"thermal-nursing","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"thermal-nursing","executionInfo":{"status":"ok","timestamp":1642519656379,"user_tz":-60,"elapsed":10511,"user":{"displayName":"KNOTS","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEorsxhplCeVDMb5ALqZ2xJWG-NY8iKO4ZWlgF=s64","userId":"07911031499750462727"}},"outputId":"591e19e8-9fa0-47e5-bc21-086a881258de"},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading collection 'popular'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cmudict.zip.\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/genesis.zip.\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/inaugural.zip.\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/names.zip.\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/stopwords.zip.\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/treebank.zip.\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/omw.zip.\n","[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/omw-1.4.zip.\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet.zip.\n","[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet2021.zip.\n","[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet31.zip.\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/words.zip.\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection popular\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}],"source":["nltk.download('popular')"]},{"cell_type":"code","execution_count":null,"id":"downtown-delhi","metadata":{"id":"downtown-delhi"},"outputs":[],"source":["from nltk.tokenize import sent_tokenize\n","# sent_tokenize is used to tokenize phrases"]},{"cell_type":"code","execution_count":null,"id":"southwest-fruit","metadata":{"id":"southwest-fruit"},"outputs":[],"source":["text_raw = 'Oggi sono stato a casa di Elisa. Non mi aveva avvertito in tempo ma per fortuna avevo giorno libero. L’ho abbracciata ma era un po’ scostante.'\n","\n","sentence_tokenizer_output = sent_tokenize(text_raw)"]},{"cell_type":"code","execution_count":null,"id":"scheduled-preserve","metadata":{"id":"scheduled-preserve","outputId":"4f334834-8b6e-4ad7-80f9-0ae7ef1ebc23"},"outputs":[{"name":"stdout","output_type":"stream","text":["Output data (via Sentence Tokenizer):\n","\tOggi sono stato a casa di Elisa.\n","\tNon mi aveva avvertito in tempo ma per fortuna avevo giorno libero.\n","\tL’ho abbracciata ma era un po’ scostante.\n"]}],"source":["print('Output data (via Sentence Tokenizer):') \n","for token in sentence_tokenizer_output:    \n","    print('\\t{}'.format(token))"]},{"cell_type":"code","execution_count":null,"id":"strange-greenhouse","metadata":{"id":"strange-greenhouse"},"outputs":[],"source":["from nltk.tokenize import word_tokenize # word_tokenize divides the paragraph in words\n","word_tokenizer_output = word_tokenize(text_raw)"]},{"cell_type":"code","execution_count":null,"id":"manufactured-norway","metadata":{"id":"manufactured-norway","outputId":"ebd9fdc2-75c8-4000-9900-529485c5c062"},"outputs":[{"data":{"text/plain":["['Oggi',\n"," 'sono',\n"," 'stato',\n"," 'a',\n"," 'casa',\n"," 'di',\n"," 'Elisa',\n"," '.',\n"," 'Non',\n"," 'mi',\n"," 'aveva',\n"," 'avvertito',\n"," 'in',\n"," 'tempo',\n"," 'ma',\n"," 'per',\n"," 'fortuna',\n"," 'avevo',\n"," 'giorno',\n"," 'libero',\n"," '.',\n"," 'L',\n"," '’',\n"," 'ho',\n"," 'abbracciata',\n"," 'ma',\n"," 'era',\n"," 'un',\n"," 'po',\n"," '’',\n"," 'scostante',\n"," '.']"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["word_tokenizer_output"]},{"cell_type":"code","execution_count":null,"id":"relative-parks","metadata":{"id":"relative-parks"},"outputs":[],"source":["# Tokenizer white space"]},{"cell_type":"code","execution_count":null,"id":"loved-strategy","metadata":{"id":"loved-strategy","outputId":"b7648a5e-a184-4bca-9571-30ed85e2ed4f"},"outputs":[{"name":"stdout","output_type":"stream","text":["['Oggi', 'sono', 'stato', 'a', 'casa', 'di', 'Elisa.', 'Non', 'mi', 'aveva', 'avvertito', 'in', 'tempo', 'ma', 'per', 'fortuna', 'avevo', 'giorno', 'libero.', 'L’ho', 'abbracciata', 'ma', 'era', 'un', 'po’', 'scostante.']\n"]}],"source":["from nltk.tokenize import WhitespaceTokenizer #  WhitespaceTokenizer divides the paragraph in words\n","Tokenizer = WhitespaceTokenizer()\n","print(Tokenizer.tokenize(text_raw))"]},{"cell_type":"code","execution_count":null,"id":"refined-trash","metadata":{"id":"refined-trash"},"outputs":[],"source":["# Tokenizer punctuation"]},{"cell_type":"code","execution_count":null,"id":"duplicate-williams","metadata":{"id":"duplicate-williams","outputId":"6c689046-1681-4926-832b-d90cf7bef051"},"outputs":[{"name":"stdout","output_type":"stream","text":["['Oggi', 'sono', 'stato', 'a', 'casa', 'di', 'Elisa', '.', 'Non', 'mi', 'aveva', 'avvertito', 'in', 'tempo', 'ma', 'per', 'fortuna', 'avevo', 'giorno', 'libero', '.', 'L', '’', 'ho', 'abbracciata', 'ma', 'era', 'un', 'po', '’', 'scostante', '.']\n"]}],"source":["from nltk.tokenize import WordPunctTokenizer # WordPunctTokenizer segments the sentence just by punctuation (. ; , / '' e.g.)\n","#text=\"\"\n","Tokenizer=WordPunctTokenizer()\n","print(Tokenizer.tokenize(text_raw))"]},{"cell_type":"code","execution_count":null,"id":"introductory-smile","metadata":{"id":"introductory-smile"},"outputs":[],"source":["# Custom tokenizer "]},{"cell_type":"code","execution_count":null,"id":"established-accountability","metadata":{"id":"established-accountability"},"outputs":[],"source":["from nltk.tokenize import RegexpTokenizer"]},{"cell_type":"code","execution_count":null,"id":"monetary-desktop","metadata":{"id":"monetary-desktop"},"outputs":[],"source":["custom_tokenizer = RegexpTokenizer('\\w+')"]},{"cell_type":"code","execution_count":null,"id":"injured-volleyball","metadata":{"id":"injured-volleyball","outputId":"0105e0a2-03e0-4649-ef93-2b9df5ee89aa"},"outputs":[{"data":{"text/plain":["['Oggi',\n"," 'sono',\n"," 'stato',\n"," 'a',\n"," 'casa',\n"," 'di',\n"," 'Elisa',\n"," 'Non',\n"," 'mi',\n"," 'aveva',\n"," 'avvertito',\n"," 'in',\n"," 'tempo',\n"," 'ma',\n"," 'per',\n"," 'fortuna',\n"," 'avevo',\n"," 'giorno',\n"," 'libero',\n"," 'L',\n"," 'ho',\n"," 'abbracciata',\n"," 'ma',\n"," 'era',\n"," 'un',\n"," 'po',\n"," 'scostante']"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["custom_tokenizer_output = custom_tokenizer.tokenize(text_raw)\n","custom_tokenizer_output"]},{"cell_type":"code","execution_count":null,"id":"casual-edwards","metadata":{"id":"casual-edwards","outputId":"96298b38-1188-49e9-bbf7-44c427fb23b7"},"outputs":[{"data":{"text/plain":["['La', 'città', 'di', 'La_Spezia', 'è', 'in', 'Liguria']"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["from nltk.tokenize import MWETokenizer\n","text = 'La città di La Spezia è in Liguria'\n","tokenizer = MWETokenizer()\n","tokenizer.add_mwe(('La', 'Spezia'))\n","tokenized_text = tokenizer.tokenize(text.split())\n","tokenized_text"]},{"cell_type":"code","execution_count":null,"id":"stable-angel","metadata":{"id":"stable-angel"},"outputs":[],"source":["# difference between Split and Tokenize "]},{"cell_type":"code","execution_count":null,"id":"chemical-wireless","metadata":{"id":"chemical-wireless","outputId":"5457d819-97d3-4751-d34d-dfa388d428c9"},"outputs":[{"name":"stdout","output_type":"stream","text":["This is the first line of text.\n","This is the second line of text.\n","['This is the first line of text.', 'This is the second line of text.']\n","['This is the first line of text.\\nThis is the second line of text.']\n","['Thi', ' i', ' the fir', 't line of text.\\nThi', ' i', ' the ', 'econd line of text.']\n","['This', 'is', 'the', 'first', 'line', 'of', 'text.', 'This', 'is', 'the', 'second', 'line', 'of', 'text.']\n","['This', 'is', 'the', 'first', 'line', 'of', 'text', '.', 'This', 'is', 'the', 'second', 'line', 'of', 'text', '.']\n"]}],"source":["from nltk.tokenize import word_tokenize\n","text=\"This is the first line of text.\\nThis is the second line of text.\"\n","print(text)\n","print(text.split('\\n')) #Spliting the text by '\\n'.\n","print(text.split('\\t')) #Spliting the text by '\\t'.\n","print(text.split('s'))  #Spliting by charecter 's'.\n","print(text.split())  #Spliting the text by space.\n","print(word_tokenize(text))    #Tokenizing by using word"]},{"cell_type":"markdown","id":"phantom-conversation","metadata":{"id":"phantom-conversation"},"source":["# Stop Words"]},{"cell_type":"code","execution_count":null,"id":"iraqi-vietnam","metadata":{"scrolled":true,"id":"iraqi-vietnam","outputId":"533516e6-fab7-4fd9-9570-55cb43b97868"},"outputs":[{"data":{"text/plain":["['Oggi',\n"," 'stato',\n"," 'casa',\n"," 'Elisa',\n"," '.',\n"," 'Non',\n"," 'avvertito',\n"," 'tempo',\n"," 'fortuna',\n"," 'giorno',\n"," 'libero',\n"," '.',\n"," 'L',\n"," '’',\n"," 'abbracciata',\n"," 'po',\n"," '’',\n"," 'scostante',\n"," '.']"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","\n","filtered_words = [token for token in word_tokenizer_output if token not in stopwords.words('italian')]\n","filtered_words"]},{"cell_type":"code","execution_count":null,"id":"taken-florida","metadata":{"id":"taken-florida"},"outputs":[],"source":["## adding custom stopwords "]},{"cell_type":"code","execution_count":null,"id":"organizational-sphere","metadata":{"id":"organizational-sphere"},"outputs":[],"source":["stopwords_list = stopwords.words('italian')\n","\n","new_stopwords = ['il', 'figa']\n","stopwords_list.extend(new_stopwords)"]},{"cell_type":"markdown","id":"brown-error","metadata":{"id":"brown-error"},"source":["# Stemming"]},{"cell_type":"markdown","id":"virgin-medium","metadata":{"id":"virgin-medium"},"source":["There are different types of stemmer in NLTK, SnowballStemmer, PorterStemmer, LancasterStemmer\n","\n","PorterStemmer:\n","\n","it was made by Martin Porter in the 1980. Its is based on a series of sequential rules.\n","\n","SnowballStemmer:\n","is an updated version of Porter’s Stemmer with new rules.\n","Snowball also contains rules for other languages. try to prevent over-stemming compared to Porter\n","\n","Lancaster stemmer:\n","Developped by Chris Paice from Lancaster University in 1990. A bit more aggressive than the previous one because the implemented rules tend to shortner the words as much as possible. "]},{"cell_type":"code","execution_count":null,"id":"narrow-temple","metadata":{"id":"narrow-temple"},"outputs":[],"source":["# import the stemmer from NLTK\n","from nltk.stem import SnowballStemmer"]},{"cell_type":"code","execution_count":null,"id":"gross-destiny","metadata":{"id":"gross-destiny","outputId":"02149c5e-3a13-4769-bc56-d1f3d974d915"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","= Quick stemming example =\n","Input words: ['leggere', 'leggo', 'lessi', 'bambino', 'bambini', 'bambina', 'bambine']\n","Corresponding stemmed words:\n","\t- legg\n","\t- legg\n","\t- less\n","\t- bambin\n","\t- bambin\n","\t- bambin\n","\t- bambin\n"]}],"source":["# create an instance of the Snowball stemmer, optimised for the Italian language\n","stemmer_snowball = SnowballStemmer('italian')\n","# create two example word-lists\n","#eg1 = ['correre', 'corro', 'corriamo', 'correremo']\n","eg1 = ['leggere', 'leggo', 'lessi']\n","eg2 = ['bambino', 'bambini', 'bambina', 'bambine']\n","# merge the lists together\n","eg_list = []\n","eg_list.extend(eg1)\n","eg_list.extend(eg2)\n","# do a for loop on both lists, apply the stemmer to each word and print the result\n","print('\\n= Quick stemming example =')\n","print('Input words: {}\\nCorresponding stemmed words:'.format(eg_list))\n","for word in eg_list:\n","    print('\\t- {}'.format(stemmer_snowball.stem(word)))"]},{"cell_type":"code","execution_count":null,"id":"corporate-gardening","metadata":{"id":"corporate-gardening","outputId":"b7cb95bd-d850-4e0c-be23-8020a8d89cb2"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Input raw text is: \n","Ogni giorno mi sveglio alle 7. La domenica alle 9 per non perdere abitudine. \n","  \n"," \n","\n"]}],"source":["from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n","from nltk.tokenize import RegexpTokenizer\n","# ### Prepare input data ###\n","# read data from file\n","filename = 'text_run_ita.txt'\n","with open(filename, 'r') as reader:\n","    input_raw_text = reader.read()\n","    print('\\nInput raw text is: \\n{}'.format(input_raw_text))\n"]},{"cell_type":"code","execution_count":null,"id":"republican-wells","metadata":{"id":"republican-wells","outputId":"914009f1-23a3-4ef2-b1eb-3ff6d8b02bf1"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Stemming 14 words with Porter, Snowball and Lancaster respectively.\n","Ogni: ogni, ogni, ogn\n","giorno: giorno, giorn, giorno\n","mi: mi, mi, mi\n","sveglio: sveglio, svegl, sveglio\n","alle: all, alle, al\n","7: 7, 7, 7\n","La: La, la, la\n","domenica: domenica, domen, domenic\n","alle: all, alle, al\n","9: 9, 9, 9\n","per: per, per, per\n","non: non, non, non\n","perdere: perder, perd, perd\n","abitudine: abitudin, abitudin, abitudin\n"]}],"source":["# split data into words\n","custom_tokenizer = RegexpTokenizer('\\w+')\n","word_list = custom_tokenizer.tokenize(input_raw_text)\n","# ### Stemming ###\n","# Create Stemmer instances\n","stemmer_porter = PorterStemmer()\n","stemmer_lancaster = LancasterStemmer()\n","stemmer_snowball = SnowballStemmer('italian')\n","# stem each word and print results\n","print('\\nStemming {} words with Porter, Snowball and Lancaster respectively.'.format(len(word_list)))\n","for word in word_list:\n","    print('{0}: {1}, {2}, {3}'.format(\n","        word,\n","        stemmer_porter.stem(word),\n","        stemmer_snowball.stem(word),\n","        stemmer_lancaster.stem(word))\n",")"]},{"cell_type":"code","execution_count":null,"id":"clinical-prague","metadata":{"id":"clinical-prague"},"outputs":[],"source":["# remove duplicate\n","word_list_unique = list(set(word_list))"]},{"cell_type":"markdown","id":"potential-microwave","metadata":{"id":"potential-microwave"},"source":["# Lemmatization"]},{"cell_type":"code","execution_count":null,"id":"opposed-barcelona","metadata":{"id":"opposed-barcelona","outputId":"8b79e487-90dd-42a2-85bd-d71a90b7750d"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to /home/mypc/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw to /home/mypc/nltk_data...\n","[nltk_data]   Package omw is already up-to-date!\n"]},{"name":"stdout","output_type":"stream","text":["\n","=== Get a lemma quickly ===\n","\n","Input word: \tleggevo\n","\tOutput Lemma: \tleggevo\n","\tOutput Stem: \tlegg\n","\n","Input word: \tlessi\n","\tOutput Lemma: \tlessi\n","\tOutput Stem: \tless\n","\n"]}],"source":["from nltk.stem import WordNetLemmatizer, SnowballStemmer\n","nltk.download('wordnet')\n","nltk.download('omw')\n","print('\\n=== Get a lemma quickly ===\\n')\n","first_lemmatizer = WordNetLemmatizer()\n","usual_stemmer = SnowballStemmer('italian')\n","input_wordlist = ['leggevo', 'lessi']\n","for word in input_wordlist:\n","    lemma = first_lemmatizer.lemmatize(word)\n","    stem = usual_stemmer.stem(word)\n","    print('Input word: \\t{}'.format(word))\n","    print('\\tOutput Lemma: \\t{}'.format(lemma))\n","    print('\\tOutput Stem: \\t{}\\n'.format(stem))"]},{"cell_type":"code","execution_count":null,"id":"focused-working","metadata":{"id":"focused-working"},"outputs":[],"source":["# Spelling correction"]},{"cell_type":"code","execution_count":null,"id":"virgin-beaver","metadata":{"id":"virgin-beaver","outputId":"6ac6192c-c83f-42d1-8e68-746072c6772a"},"outputs":[{"data":{"text/plain":["['confusion', 'matrix']"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["from spellchecker import SpellChecker\n","def spell_check(text):\n","    \n","    result = []\n","    spell = SpellChecker()\n","    for word in text:\n","        correct_word = spell.correction(word)\n","        result.append(correct_word)\n","    \n","    return result\n","\n","#Test\n","text = \"confuson matrx\".split() \n","spell_check(text)\n","\n"]},{"cell_type":"markdown","source":["# Exercise"],"metadata":{"id":"ewWvPiBNzfca"},"id":"ewWvPiBNzfca"},{"cell_type":"markdown","source":["\n","\n","1.   Read a text file called\n","2.   Count the number of sentences in the document\n","3.   Create the vocabulary of tokens and words\n","4.   Use the lemmatizer to reduce the vocabulary and compare the length \n","5.   Read the document and compute the vocabulary \n","6.   Compute the number of OOV elements\n","7.   Drop out stopwords from the vocabulary and compute again the OOV\n"],"metadata":{"id":"l_4EX_H91rUZ"},"id":"l_4EX_H91rUZ"},{"cell_type":"code","execution_count":null,"id":"aggressive-aruba","metadata":{"id":"aggressive-aruba"},"outputs":[],"source":["\n"]},{"cell_type":"code","execution_count":null,"id":"heard-gabriel","metadata":{"id":"heard-gabriel"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"name":"Day_1_NLTK.ipynb","provenance":[],"collapsed_sections":["brown-error"]}},"nbformat":4,"nbformat_minor":5}